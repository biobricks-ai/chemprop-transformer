\section{Introduction}

In recent years, machine learning (ML) has revolutionized the field of molecular property prediction, with significant implications for drug discovery and toxicological analysis. Modern techniques focus on learning representations directly from molecular structures, moving beyond traditional approaches that relied on predefined chemical descriptors or hand-derived geometric features. This shift has led to more flexible and powerful models capable of capturing complex molecular properties.

Transformer models, which have demonstrated remarkable success in natural language processing tasks, have recently been applied to molecular property prediction and generation \cite{molformer}. These models, capable of capturing long-range dependencies in sequential data, show promise in learning complex molecular representations. Our approach builds upon this foundation, utilizing a transformer architecture to process molecular structures and their associated properties.

Our work draws significant inspiration from the realization that large-language-models can act as unsupervised multitask models \cite{llms-are-unsuper-learners}, that they can act as performant classification models without training for specific tasks, and that, when fine-tuned, they can enhance this performance on specific tasks \cite{zero-shot-learners,bert-pretraining}. This insight led us to explore the potential of transformers in the domain of molecular property prediction. We hypothesized that by leveraging sequences of property-value pairs, we could create a model capable of making 'conditional' predictions and improving performance on sparse endpoints by enabling transfer learning from other, more abundant endpoints.

The key innovation in our approach lies in its ability to generate conditional predictions. For instance, our model can answer questions like "What would be the value of property Y for chemical X if chemical X had a specific value for property Z?" 

It's important to note that the current work serves as a proof of concept, demonstrating the viability of this approach on a dataset reduced to binary (positive/negative) classifications. However, the potential applications of this methodology are far-reaching. Future iterations of the model could be capable of generating text-based outputs such as chemical descriptions, numeric outputs for regression tasks, categorical outputs like UN GHS hazard categories, and even capturing complex syntactical structures like chemical reactions and pathway information. The Matteo et al. work on a 'regression transformer' already demonstrates that these tasks can be effectively modeled by transformer based models, and this work demonstrates that those methods are likely to extend to the multi-task setting \cite{regression-transformer}. This versatility positions the approach as a powerful tool for a wide range of tasks in cheminformatics and related fields.

In this work, data from the ChemHarmony dataset is used to provide training and evaluation data in the form of a large table of molecular structures, paired with property indices and binary values (positive or negative) \cite{biobricks}. \todo{how big is chemharmony now? where does it source data from?}

During training, tensors are created for molecules by tokenizing their SELFIES representation \cite{selfies}, tensors are created from properties by assigning them unique indices from which the model creates embeddings, and tensors are created for labels by assigning them one of two possible tokens (positive or negative) which are embedded by the model. These tensors are combined to create an input from the tokenized selfies, and an output by concatenating pairs of property tokens and value tokens.

The main contributions of this paper are:

\begin{itemize}
    \item Implementation of a multi-task learning framework that allows simultaneous prediction of multiple molecular properties.
    \item Demonstration of the improved performance on popular benchmarks when the model is given conditional information about unseen molecules.
    \item Demonstration of the possibility to use the model to do conditional inference and act as a tool for designing integrating testing strategies.
\end{itemize}

Our approach combines the strengths of efficient molecular representation with the power of transformer models, addressing some of the limitations of previous methods. By training on a large, diverse dataset of molecular structures and their associated properties, we aim to create a versatile model capable of accurately predicting molecular properties across a wide range of tasks. This model leverages the relationships between different properties to enhance prediction accuracy and enable conditional inference, providing a powerful tool for molecular property analysis and prediction.

In the following sections, we detail our methodology, present the results of our experiments, and discuss the implications of our findings for the field of toxicology. We also compare our model's performance with existing baselines, including graph neural networks and other language model-based approaches, to demonstrate its effectiveness in capturing molecule-property relationships.