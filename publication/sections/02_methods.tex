\section{Methods}

\subsection{Dataset}

Our study utilizes the ChemHarmony dataset, a comprehensive collection of chemical compounds and their associated bioactivity data. This dataset provides a rich source of information for training our model, including molecular structures and corresponding biological activity across various assays.

\subsection{Molecular Representation}

We employ the SELFIES (SELF-referencing Embedded Strings) representation for encoding molecular structures. SELFIES offers several advantages over traditional representations:

\begin{itemize}
    \item Robustness: SELFIES ensures that all generated strings correspond to valid molecular graphs.
    \item Completeness: It can represent any organic molecule composed of main-group elements.
    \item Simplicity: The representation is easily interpretable and manipulable.
\end{itemize}

\subsection{Model Architecture}

Our model is based on a Conditional Variational Autoencoder (CVAE) architecture. The CVAE consists of three main components:

\begin{enumerate}
    \item Encoder: Transforms the input SELFIES representation into a latent space representation.
    \item Decoder: Reconstructs the SELFIES representation from the latent space.
    \item Condition Network: Incorporates the target property information into the latent space.
\end{enumerate}

The encoder and decoder are implemented as recurrent neural networks (RNNs) with LSTM cells, while the condition network is a feedforward neural network.

\subsection{Training Process}

The model is trained using the following steps:

\begin{enumerate}
    \item Preprocessing: Convert molecules to SELFIES representation and encode bioactivity data.
    \item Batching: Create mini-batches of molecule-property pairs.
    \item Forward Pass: Encode molecules, sample from latent space, and decode.
    \item Loss Calculation: Compute reconstruction loss and KL divergence.
    \item Backpropagation: Update model parameters using gradient descent.
\end{enumerate}

We use the Adam optimizer and implement KL annealing to balance reconstruction and KL divergence terms during training.

\subsection{Molecule Generation}

To generate new molecules with desired properties:

\begin{enumerate}
    \item Sample from the prior distribution in latent space.
    \item Condition the latent vector with the target property.
    \item Decode the conditioned latent vector to obtain a SELFIES string.
    \item Convert the SELFIES string back to a molecular structure.
\end{enumerate}

\subsection{Evaluation Metrics}

We evaluate our model using the following metrics:

\begin{itemize}
    \item Chemical Validity: Percentage of generated molecules that are chemically valid.
    \item Novelty: Percentage of generated molecules not present in the training set.
    \item Diversity: Distribution of Tanimoto similarities between generated molecules.
    \item Property Prediction Accuracy: Ability to generate molecules with desired bioactivity profiles.
\end{itemize}

These methods allow us to train a robust CVAE model capable of generating novel, diverse, and property-targeted molecular structures based on the ChemHarmony dataset.
