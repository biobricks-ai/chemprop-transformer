@article{zero-shot-learners,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{bert-pretraining,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{llms-are-unsuper-learners,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{molformer,
  title={Large-scale chemical language representations capture molecular structure and properties},
  author={Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},
  journal={Nature Machine Intelligence},
  volume={4},
  number={12},
  pages={1256--1264},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{selfies,
  title={Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation},
  author={Krenn, Mario and H{\"a}se, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alan},
  journal={Machine Learning: Science and Technology},
  volume={1},
  number={4},
  pages={045024},
  year={2020},
  publisher={IOP Publishing}
}

@article{regression-transformer,
  title={Regression transformer enables concurrent sequence regression and generation for molecular language modelling},
  author={Born, Jannis and Manica, Matteo},
  journal={Nature Machine Intelligence},
  volume={5},
  number={4},
  pages={432--444},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{biobricks,
  title={BioBricks. ai: A Versioned Data Registry for Life Sciences Data Assets},
  author={Gao, Yifan and Mughal, Zakariyya and Jaramillo-Villegas, Jose A and Corradi, Marie and Borrel, Alexandre and Lieberman, Ben and Sharif, Suliman and Shaffer, John and Fecho, Karamarie and Chatrath, Ajay and others},
  journal={arXiv preprint arXiv:2408.17320},
  year={2024}
}

% MODEL COMPARISONS ===========================================================

@article{molformer_25,
  title={ChemBERTa: large-scale self-supervised pretraining for molecular property prediction},
  author={Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  journal={arXiv preprint arXiv:2010.09885},
  year={2020}
}

@article{molformer_26,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{molformer_32,
  title={Strategies for pre-training graph neural networks},
  author={Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1905.12265},
  year={2019}
}

@article{molformer_33,
  title={N-gram graph: Simple unsupervised representation for graphs, with applications to molecules},
  author={Liu, Shengchao and Demirel, Mehmet F and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{molformer_36,
  title={Pre-training molecular graph representation with 3d geometry},
  author={Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  journal={arXiv preprint arXiv:2110.07728},
  year={2021}
}

@article{molformer_37,
  title={Directional message passing for molecular graphs},
  author={Gasteiger, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2003.03123},
  year={2020}
}

@article{molformer_38,
  title={Geometry-enhanced molecular representation learning for property prediction},
  author={Fang, Xiaomin and Liu, Lihang and Lei, Jieqiong and He, Donglong and Zhang, Shanzhuo and Zhou, Jingbo and Wang, Fan and Wu, Hua and Wang, Haifeng},
  journal={Nature Machine Intelligence},
  volume={4},
  number={2},
  pages={127--134},
  year={2022},
  publisher={Nature Publishing Group}
}

@inproceedings{molformer_56,
  title={Molecular property prediction: A multilevel quantum interactions modeling perspective},
  author={Lu, Chengqiang and Liu, Qi and Wang, Chao and Huang, Zhenya and Lin, Peize and He, Lixin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={1052--1060},
  year={2019}
}

@article{molformer_57,
  title={Analyzing learned molecular representations for property prediction},
  author={Yang, Kevin and Swanson, Kyle and Jin, Wengong and Coley, Connor and Eiden, Philipp and Gao, Hua and Guzman-Perez, Angel and Hopper, Timothy and Kelley, Brian and Mathea, Miriam and others},
  journal={Journal of chemical information and modeling},
  volume={59},
  number={8},
  pages={3370--3388},
  year={2019},
  publisher={ACS Publications}
}